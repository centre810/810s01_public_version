{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Fichier qui permet d'exporter les differentes donnees necessaires a la diffusion des graphs sur le web, via quelques fonctions qui les retraitent pour correspondre aux filtres attendus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported 32 channel data\n"
     ]
    }
   ],
   "source": [
    "import gen_dataset_810 as mod_810fr\n",
    "# importer datetime pour typer les donnees de dates des chaines et des videos\n",
    "import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# Init YouTube Data\n",
    "datemin = datetime.datetime(2021, 1, 1, 0, 0, 0)\n",
    "folder_candidats = './data_sources/stats_videos/2022_03_08/'\n",
    "dt_candidats = mod_810fr.mb_ds()\n",
    "videos_candidats, chaines_candidats  = dt_candidats.get_full_data_candidat(folder = folder_candidats)\n",
    "videos_candidats = videos_candidats[videos_candidats['dateVideo'] > datemin]\n",
    "\n",
    "# Transcript files folder\n",
    "transcripts_folder = \"./data_sources/transcripts_videos/\"\n",
    "\n",
    "# Output folders\n",
    "folder_json = './data_output/wordCloud-json/'\n",
    "folder_csv = './data_output/wordCloud-csv/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Fonctions et paramètres globaux\n",
    "\n",
    "## 1.1. Liste d'exceptions en plus de NLTK default\n",
    "\n",
    "Liste des mots qu'on va supprimer de nos traitements pour obtenir des \"nuages de mots\" plus clairs\n",
    "\n",
    "_N.B : On ne fait pas de nuages parce qu'on n'aime pas trop le style_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste exclusion mot commune\n",
    "liste_exclusion_additionnelle = ['d', \n",
    "                'du', \n",
    "                'de', \n",
    "                'la', \n",
    "                'des', \n",
    "                'le', \n",
    "                'et', \n",
    "                'est', \n",
    "                'elle', \n",
    "                'une', \n",
    "                'en', \n",
    "                'que', \n",
    "                'aux', \n",
    "                'qui', \n",
    "                'ces', \n",
    "                'les', \n",
    "                'dans', \n",
    "                'sur', \n",
    "                'l', \n",
    "                'un', \n",
    "                'pour', \n",
    "                'par', \n",
    "                'il', \n",
    "                'ou', \n",
    "                'à', \n",
    "                'ce', \n",
    "                'a', \n",
    "                'sont', \n",
    "                'cas', \n",
    "                'plus', \n",
    "                'leur', \n",
    "                'se', \n",
    "                's', \n",
    "                'vous', \n",
    "                'au', \n",
    "                'c', \n",
    "                'aussi', \n",
    "                'toutes', \n",
    "                'autre', \n",
    "                'comme', \n",
    "                \"c'est\",\n",
    "                \"parce\",\n",
    "                \"ya\",\n",
    "                \"ça\", \n",
    "                \"donc\",\n",
    "                \"où\",\n",
    "                \"notamment\",\n",
    "                \"alors\",\n",
    "                \"cette\",\n",
    "                \"cet\",\n",
    "                \"si\", \n",
    "                \"dis\",\n",
    "                \"dit\",\n",
    "                \"dire\",\n",
    "                \"fait\", \n",
    "                \"faire\", \n",
    "                \"fais\",\n",
    "                \"car\", \n",
    "                \"là\",\n",
    "                'non',\n",
    "                'tous',\n",
    "                'puis',\n",
    "                'cela',\n",
    "                'abord',\n",
    "                'quand',\n",
    "                'après',\n",
    "                'très',\n",
    "                'entre',\n",
    "                'tout',\n",
    "                'voilà',\n",
    "                'tous',\n",
    "                'oui',\n",
    "                'va',\n",
    "                'ans',\n",
    "                'veux',\n",
    "                'question',\n",
    "                'hui',\n",
    "                'avoir',\n",
    "                'eric',\n",
    "                'emmanuel',\n",
    "                'yannick',\n",
    "                'valérie',\n",
    "                'hui', 'tout', 'leurs', 'dix', 'très',\n",
    "                'jean', 'luc', 'ceux', 'celles',\n",
    "                'peut', 'faut', 'cinq', '35', 'veut', 'peux', 'veux',\n",
    "                'vais', 'bien', 'mal', 'fois',\n",
    "                'pourquoi', 'comment',\n",
    "                'beaucoup', 'peu', 'deux',\n",
    "                'monsieur', 'messieurs', 'madame', 'mesdames',\n",
    "                'chose', 'être', 'gens',\n",
    "                'depuis', 'encore',\n",
    "                'aujourd',\n",
    "                'autre',\n",
    "                'quoi',\n",
    "                'applaudissements'\n",
    "                ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Fonctions nécessaires à nos différents traitements\n",
    "\n",
    "### 1.2.1. Récupération des fichiers de transcripts et transformation de ces derniers en une variable de texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "import ast\n",
    "\n",
    "\n",
    "# Dump des fichiers associé à chacun des candidats \n",
    "def dump_liste_videos_par_candidat(dtsource, liste_candidats = [], periode_de_temps = {}, complement_titre = '') :\n",
    "    cnt = 0\n",
    "    for candidat in liste_candidats :\n",
    "        subdtf_candidat = dtsource[dtsource['candidat'] == candidat]\n",
    "        subdtf_candidat = subdtf_candidat[subdtf_candidat['dateVideo'] > periode_de_temps['datemin']]\n",
    "        subdtf_candidat = subdtf_candidat[subdtf_candidat['dateVideo'] < periode_de_temps['datemax']]\n",
    "        subdtf_candidat.to_csv(dossier_source+nom_fichier+\"_\"+str(cnt)+\".csv\") #+\"_\"+ complement_titre \n",
    "        cnt += 1\n",
    "\n",
    "    return\n",
    "\n",
    "# Vérifier la présence des transcripts pour les vidéos des candidats\n",
    "def check_exhau_transcript(dtf_candidat, dossier_ts_associe = './data_sources/transcripts_videos/') : \n",
    "    data_manquante = []\n",
    "    data_presente = []\n",
    "\n",
    "    dtf_videos_candidat = dtf_candidat\n",
    "    liste_videos_candidats = dtf_videos_candidat.vid_id.unique()\n",
    "    for video_id in liste_videos_candidats : \n",
    "        fichier_ts = Path(dossier_ts_associe+\"/\"+ video_id+\".json\")\n",
    "        if Path.is_file(fichier_ts) :\n",
    "            data_presente.append(video_id)\n",
    "        else :\n",
    "            # TODO : Dropper aussi le titre de la video pour la retrouver plus facilement\n",
    "            data_manquante.append(video_id)\n",
    "    \n",
    "    print(str(len(data_manquante)) + \"/\" +str(dtf_candidat.shape[0]) + \" n'ont pas de transcript. Peut-être les sous-titres furent-ils désactivés\")\n",
    "\n",
    "    return data_manquante, data_presente\n",
    "\n",
    "# A partir du nom d'un candidat, on retrouve le nom du dossier \n",
    "#   -> via l'expression reguliere utilisée pour créer son dossier à la base (pas de signes diacritiques)\n",
    "def get_dossier_candidat(nom = '') : \n",
    "    clean_name = re.sub(r\"[^a-zA-Z0-9]\",\"\",nom)\n",
    "    return clean_name\n",
    "\n",
    "# Les fichiers de transcripts, même s'ils s'appellent json sont en fait, des fichiers qui contiennent des éléments sous la forme de\n",
    "# listes de dictionnaires. C'est ce que la fonction retournera pour un fichier.\n",
    "def get_transcript_as_list(nomfichier = '') : \n",
    "    with open(nomfichier) as f:\n",
    "        data = ast.literal_eval(f.read())\n",
    "    return data\n",
    "\n",
    "# Retourne une variable de texte sur la base du fichier de transcript passé en paramètre.\n",
    "# N.B : Tous les passages à la ligne sont transformés en espaces\n",
    "def create_text_variable_for_transcript(nomfichier = '') :\n",
    "    liste_data = get_transcript_as_list(nomfichier=nomfichier)\n",
    "    text_result = ''\n",
    "    for i in range(len(liste_data)) :\n",
    "        try :\n",
    "            text_sequence = liste_data[i]['text']\n",
    "            text_sequence = text_sequence.replace('\\n', ' ')\n",
    "            text_result += ' ' + text_sequence\n",
    "        except KeyError :\n",
    "            print(\"Exception dans la recherche de texte\")\n",
    "            continue\n",
    "    \n",
    "    return text_result\n",
    "\n",
    "# Crée, pour un candidat, une seule variable de texte composée du texte issu du transcript de toutes les vidéos\n",
    "# J'avais un peu peur qu'on puisse overflower les strings python, mais si j'en crois ce site : https://www.tutorialspoint.com/What-is-the-max-length-of-a-Python-string\n",
    "# On est laaaaaaarge\n",
    "# En gros a l'issue de cette fonction, on a un très gros texte qui est le texte du transcript de toutes les vidéos des candidats, mis bout à bout\n",
    "def creer_logorrhee_candidat(nom = '', dossierparent= '', liste_vidid_arecup = []) :\n",
    "    # Defini le dossier ou trouver les fichiers de transcript\n",
    "    nomdossier = get_dossier_candidat(nom=nom)\n",
    "    dossier_candidat = dossierparent + \"/\" + nomdossier\n",
    "    logorrhee = ''\n",
    "\n",
    "    # Defini la liste des videos dont on veut recuperer le transcript\n",
    "    for vid_id in liste_vidid_arecup :\n",
    "        filename = vid_id + '.json'\n",
    "        text_tampon = create_text_variable_for_transcript(nomfichier=dossier_candidat+\"/\"+filename)\n",
    "        logorrhee += ' ' + text_tampon.lower()\n",
    "\n",
    "    return logorrhee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2. Tokenization des textes générés pour faire des calculs d'occurrence des mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse de texte : NLTK = Natural Language Tool Kit\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "# Librairies pour créer les Word Cloud\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Tokenization et calcul NLTK sur la base de la grosse chaîne de texte générée\n",
    "# Parametre permet de faire une liste d'exception custom pour chaque dataset considere\n",
    "def NLTK_processing(logorrhee, liste_exception_custo = []):\n",
    "    #Expression régulière pour ne conserver que les caractères alphanumérique\n",
    "    tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "    logorrhee_tokenisee = tokenizer.tokenize(logorrhee)\n",
    "    # On enlève la liste des stopwords par défaut de NLTK\n",
    "    logorrhee_nettoyee =[t for t in logorrhee_tokenisee if t not in stopwords.words('french')]\n",
    "    logorrhee_nettoyee_custo = [t for t in logorrhee_nettoyee if t not in liste_exclusion_additionnelle]\n",
    "    if len(liste_exception_custo) > 0 :\n",
    "        logorrhee_nettoyee_custo = [t for t in logorrhee_nettoyee_custo if t not in liste_exception_custo]\n",
    "    \n",
    "    # stemmer = FrenchStemmer()\n",
    "    # logorrhee_stemmee =[stemmer.stem(w) for w in logorrhee_nettoyee]\n",
    "    # En fait, on ne peut pas trop stemmer pour faire des nuages de mots, sinon ces derniers sont tout pourri\n",
    "\n",
    "    # On recolle les mots ensemble séparés par des espaces\n",
    "    logorrhee_recollee = ' '.join([word for word in logorrhee_nettoyee_custo])\n",
    "    return logorrhee_recollee\n",
    "\n",
    "# Fonction qui pourrait nous permettre de refiltrer les les chaînes de textes\n",
    "def additionnal_cleaning(dtf, liste = []) :\n",
    "    inverse_boolean_series = ~dtf.mot.isin(liste)\n",
    "    inverse_filtered_df = dtf[inverse_boolean_series]\n",
    "    return inverse_filtered_df\n",
    "\n",
    "# Calcul de fréquence de mots dans une string\n",
    "def wordFrequency(string):\n",
    "    # converting the string into lowercase\n",
    "    string=string.lower()\n",
    "    # Whenever we encounter a space, break the string\n",
    "    string=string.split(\" \")\n",
    "    # Initializing a dictionary to store the frequency of words\n",
    "    word_frequency={}\n",
    "    # <a href=\"https://www.pythonpool.com/python-iterate-through-list/\" target=\"_blank\" rel=\"noreferrer noopener\">Iterating</a> through the string\n",
    "    for i in string:\n",
    "     \n",
    "    # If the word is already in the keys, increment its frequency\n",
    "        if i in word_frequency:\n",
    "            word_frequency[i]+=1\n",
    "             \n",
    "    # It means that this is the first occurence of the word\n",
    "        else:\n",
    "            word_frequency[i]=1\n",
    "    return(word_frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3. Dump d'un fichier .json au format attendu pour le site https://s01.810.fr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def dump_top_mots_candidat(dtf_synth_courant, nomfichier) :\n",
    "    liste_a_dumper = []\n",
    "    styles_possibles = [\"400\", \"700\", \"600\", '100', \"bold\", \"normal\", \"500\", \"400\", \"200\"]\n",
    "    for index, row in dtf_synth_courant.iterrows():\n",
    "        liste_a_dumper.append({\"\\\"label\\\"\" : \"\\\"\"+ str(row[\"mot\"]) + \"\\\"\", \"\\\"value\\\"\" : row[\"occurences\"], \"\\\"style\\\"\" : \"\\\"\"+ random.choice(styles_possibles) + \"\\\"\" })\n",
    "\n",
    "    liste_en_geustr = str(liste_a_dumper)\n",
    "    liste_en_geustr = liste_en_geustr.replace(\"'\",\"\")\n",
    "    with open(nomfichier, \"w\") as text_file:\n",
    "        text_file.write(liste_en_geustr)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Générer les listes de vidéos sur lesquelles on aimerait travailler / candidat.\n",
    "\n",
    "Permet d'exporter la liste des videos des candidats pour lesquelles on aimerait générer des nuages de mots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramètres globaux à mettre à jour si on change de fichier\n",
    "dossier_source = \"./temp_files/lst_vidz_for_wordcloud/\"\n",
    "# Modifier le nom du fichier avec le fichier souhaité\n",
    "nom_fichier = \"auto_candidat\"\n",
    "\n",
    "liste_candidats = ['Eric Zemmour', 'Avec vous', 'Jean-Luc Mélenchon', 'Valérie Pécresse', 'Yannick Jadot', 'Anne Hidalgo', 'Jean Lassalle']\n",
    "periode_de_temps = {'datemin' : datetime.datetime(2022, 2, 1, 0, 0, 0), 'datemax' : datetime.datetime(2022, 3, 1, 0, 0, 0)}\n",
    "\n",
    "dump_liste_videos_par_candidat(videos_candidats, liste_candidats = liste_candidats, periode_de_temps= periode_de_temps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Créer les wordcloud issus des transcripts desdites vidéos\n",
    "\n",
    "## 3.1. Eric Zemmour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/18 n'ont pas de transcript. Peut-être les sous-titres furent-ils désactivés\n"
     ]
    }
   ],
   "source": [
    "indice_candidat = 0\n",
    "fichier_vidz = dossier_source + 'auto_candidat_' + str(indice_candidat) + '.csv'\n",
    "dossier_ts = transcripts_folder + 'EricZemmour'\n",
    "df_candidat_2 = pd.read_csv(fichier_vidz)\n",
    "\n",
    "\n",
    "# Voir si les vidéos considérées ont bien des transcripts\n",
    "liste_videoid_sans_transcript, liste_videoid_avec_transcript = check_exhau_transcript(dtf_candidat=df_candidat_2, dossier_ts_associe = dossier_ts)\n",
    "# Croiser la liste des vidéos et les fichiers de transcripts pour créer la logorrhee (gros texte de tous les mots de tous les transcripts considérés)\n",
    "logoR_2 = creer_logorrhee_candidat(nom=liste_candidats[indice_candidat], dossierparent=transcripts_folder, liste_vidid_arecup = liste_videoid_avec_transcript)\n",
    "# Nettoyer la logorrhee\n",
    "cleanlog = NLTK_processing(logorrhee=logoR_2)\n",
    "\n",
    "# Nettoyer un peu les scories et faire un datafram avec [mot | nb occurences]\n",
    "data_dict = wordFrequency(cleanlog)\n",
    "data_items = data_dict. items()\n",
    "data_list = list(data_items)\n",
    "df = pd. DataFrame(data_list, columns=['mot', 'occurences']) \n",
    "\n",
    "df = df.sort_values(by = ['occurences'], ascending=False)\n",
    "df_to_export = df.head(20)\n",
    "\n",
    "# Dump des fichiers pour https://s01.810.fr\n",
    "dump_top_mots_candidat(dtf_synth_courant=df_to_export, nomfichier=folder_json+'mot_fev22-zemmour.json')\n",
    "df_to_export.to_csv(folder_csv+'mot_fev22-zemmour.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Emmanuel Macron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/7 n'ont pas de transcript. Peut-être les sous-titres furent-ils désactivés\n"
     ]
    }
   ],
   "source": [
    "indice_candidat = 1\n",
    "fichier_vidz = dossier_source + 'auto_candidat_' + str(indice_candidat) + '.csv'\n",
    "dossier_ts = transcripts_folder + '/Avecvous'\n",
    "df_candidat_2 = pd.read_csv(fichier_vidz)\n",
    "\n",
    "# Voir si les vidéos considérées ont bien des transcripts\n",
    "liste_videoid_sans_transcript, liste_videoid_avec_transcript = check_exhau_transcript(dtf_candidat=df_candidat_2, dossier_ts_associe = dossier_ts)\n",
    "# Croiser la liste des vidéos et les fichiers de transcripts pour créer la logorrhee (gros texte de tous les mots de tous les transcripts considérés)\n",
    "logoR_2 = creer_logorrhee_candidat(nom=liste_candidats[indice_candidat], dossierparent=transcripts_folder, liste_vidid_arecup = liste_videoid_avec_transcript)\n",
    "# Nettoyer la logorrhee\n",
    "cleanlog = NLTK_processing(logorrhee=logoR_2)\n",
    "\n",
    "# Nettoyer un peu les scories et faire un datafram avec [mot | nb occurences]\n",
    "data_dict = wordFrequency(cleanlog)\n",
    "data_items = data_dict. items()\n",
    "data_list = list(data_items)\n",
    "df = pd. DataFrame(data_list, columns=['mot', 'occurences']) \n",
    "\n",
    "df = df.sort_values(by = ['occurences'], ascending=False)\n",
    "df_to_export = df.head(20)\n",
    "\n",
    "# Dump des fichiers pour https://s01.810.fr\n",
    "dump_top_mots_candidat(dtf_synth_courant=df_to_export, nomfichier=folder_json+'mot_fev22-macron.json')\n",
    "df_to_export.to_csv(folder_csv+'mot_fev22-macron.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Jean-Luc Mélenchon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/27 n'ont pas de transcript. Peut-être les sous-titres furent-ils désactivés\n"
     ]
    }
   ],
   "source": [
    "indice_candidat = 2\n",
    "fichier_vidz = dossier_source + 'auto_candidat_' + str(indice_candidat) + '.csv'\n",
    "dossier_ts = transcripts_folder + 'JeanLucMlenchon'\n",
    "df_candidat_2 = pd.read_csv(fichier_vidz)\n",
    "\n",
    "# Voir si les vidéos considérées ont bien des transcripts\n",
    "liste_videoid_sans_transcript, liste_videoid_avec_transcript = check_exhau_transcript(dtf_candidat=df_candidat_2, dossier_ts_associe = dossier_ts)\n",
    "# Croiser la liste des vidéos et les fichiers de transcripts pour créer la logorrhee (gros texte de tous les mots de tous les transcripts considérés)\n",
    "logoR_2 = creer_logorrhee_candidat(nom=liste_candidats[indice_candidat], dossierparent=transcripts_folder, liste_vidid_arecup = liste_videoid_avec_transcript)\n",
    "# Nettoyer la logorrhee\n",
    "cleanlog = NLTK_processing(logorrhee=logoR_2)\n",
    "\n",
    "# Nettoyer un peu les scories et faire un datafram avec [mot | nb occurences]\n",
    "data_dict = wordFrequency(cleanlog)\n",
    "data_items = data_dict. items()\n",
    "data_list = list(data_items)\n",
    "df = pd. DataFrame(data_list, columns=['mot', 'occurences']) \n",
    "\n",
    "df = df.sort_values(by = ['occurences'], ascending=False)\n",
    "df_to_export = df.head(20)\n",
    "\n",
    "# Dump des fichiers pour https://s01.810.fr\n",
    "dump_top_mots_candidat(dtf_synth_courant=df_to_export, nomfichier=folder_json+'mot_fev22-melenchon.json')\n",
    "df_to_export.to_csv(folder_csv+'mot_fev22-melenchon.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Valérie Pécresse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/24 n'ont pas de transcript. Peut-être les sous-titres furent-ils désactivés\n"
     ]
    }
   ],
   "source": [
    "indice_candidat = 3\n",
    "fichier_vidz = dossier_source + 'auto_candidat_' + str(indice_candidat) + '.csv'\n",
    "dossier_ts = transcripts_folder + 'ValriePcresse'\n",
    "df_candidat_2 = pd.read_csv(fichier_vidz)\n",
    "\n",
    "# Voir si les vidéos considérées ont bien des transcripts\n",
    "liste_videoid_sans_transcript, liste_videoid_avec_transcript = check_exhau_transcript(dtf_candidat=df_candidat_2, dossier_ts_associe = dossier_ts)\n",
    "# Croiser la liste des vidéos et les fichiers de transcripts pour créer la logorrhee (gros texte de tous les mots de tous les transcripts considérés)\n",
    "logoR_2 = creer_logorrhee_candidat(nom=liste_candidats[indice_candidat], dossierparent=transcripts_folder, liste_vidid_arecup = liste_videoid_avec_transcript)\n",
    "# Nettoyer la logorrhee\n",
    "cleanlog = NLTK_processing(logorrhee=logoR_2)\n",
    "\n",
    "# Nettoyer un peu les scories et faire un datafram avec [mot | nb occurences]\n",
    "data_dict = wordFrequency(cleanlog)\n",
    "data_items = data_dict. items()\n",
    "data_list = list(data_items)\n",
    "df = pd. DataFrame(data_list, columns=['mot', 'occurences']) \n",
    "\n",
    "df = df.sort_values(by = ['occurences'], ascending=False)\n",
    "df_to_export = df.head(20)\n",
    "\n",
    "# Dump des fichiers pour https://s01.810.fr\n",
    "dump_top_mots_candidat(dtf_synth_courant=df_to_export, nomfichier=folder_json+'mot_fev22-pecresse.json')\n",
    "df_to_export.to_csv(folder_csv+'mot_fev22-pecresse.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5. Yannick Jadot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/36 n'ont pas de transcript. Peut-être les sous-titres furent-ils désactivés\n"
     ]
    }
   ],
   "source": [
    "indice_candidat = 4\n",
    "fichier_vidz = dossier_source + 'auto_candidat_' + str(indice_candidat) + '.csv'\n",
    "dossier_ts = transcripts_folder + 'YannickJadot'\n",
    "df_candidat_2 = pd.read_csv(fichier_vidz)\n",
    "\n",
    "# Voir si les vidéos considérées ont bien des transcripts\n",
    "liste_videoid_sans_transcript, liste_videoid_avec_transcript = check_exhau_transcript(dtf_candidat=df_candidat_2, dossier_ts_associe = dossier_ts)\n",
    "# Croiser la liste des vidéos et les fichiers de transcripts pour créer la logorrhee (gros texte de tous les mots de tous les transcripts considérés)\n",
    "logoR_2 = creer_logorrhee_candidat(nom=liste_candidats[indice_candidat], dossierparent=transcripts_folder, liste_vidid_arecup = liste_videoid_avec_transcript)\n",
    "# Nettoyer la logorrhee\n",
    "cleanlog = NLTK_processing(logorrhee=logoR_2)\n",
    "\n",
    "# Nettoyer un peu les scories et faire un datafram avec [mot | nb occurences]\n",
    "data_dict = wordFrequency(cleanlog)\n",
    "data_items = data_dict. items()\n",
    "data_list = list(data_items)\n",
    "df = pd. DataFrame(data_list, columns=['mot', 'occurences']) \n",
    "\n",
    "df = df.sort_values(by = ['occurences'], ascending=False)\n",
    "df_to_export = df.head(20)\n",
    "\n",
    "# Dump des fichiers pour https://s01.810.fr\n",
    "dump_top_mots_candidat(dtf_synth_courant=df_to_export, nomfichier=folder_json+'mot_fev22-jadot.json')\n",
    "df_to_export.to_csv(folder_csv+'mot_fev22-jadot.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6. Anne Hidalgo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/18 n'ont pas de transcript. Peut-être les sous-titres furent-ils désactivés\n"
     ]
    }
   ],
   "source": [
    "indice_candidat = 5\n",
    "fichier_vidz = dossier_source + 'auto_candidat_' + str(indice_candidat) + '.csv'\n",
    "dossier_ts = transcripts_folder + 'AnneHidalgo'\n",
    "df_candidat_2 = pd.read_csv(fichier_vidz)\n",
    "\n",
    "# Voir si les vidéos considérées ont bien des transcripts\n",
    "liste_videoid_sans_transcript, liste_videoid_avec_transcript = check_exhau_transcript(dtf_candidat=df_candidat_2, dossier_ts_associe = dossier_ts)\n",
    "# Croiser la liste des vidéos et les fichiers de transcripts pour créer la logorrhee (gros texte de tous les mots de tous les transcripts considérés)\n",
    "logoR_2 = creer_logorrhee_candidat(nom=liste_candidats[indice_candidat], dossierparent=transcripts_folder, liste_vidid_arecup = liste_videoid_avec_transcript)\n",
    "# Nettoyer la logorrhee\n",
    "cleanlog = NLTK_processing(logorrhee=logoR_2)\n",
    "\n",
    "# Nettoyer un peu les scories et faire un datafram avec [mot | nb occurences]\n",
    "data_dict = wordFrequency(cleanlog)\n",
    "data_items = data_dict. items()\n",
    "data_list = list(data_items)\n",
    "df = pd. DataFrame(data_list, columns=['mot', 'occurences']) \n",
    "\n",
    "df = df.sort_values(by = ['occurences'], ascending=False)\n",
    "df_to_export = df.head(20)\n",
    "\n",
    "# Dump des fichiers pour https://s01.810.fr\n",
    "dump_top_mots_candidat(dtf_synth_courant=df_to_export, nomfichier=folder_json+'mot_jan22-hidalgo.json')\n",
    "df_to_export.to_csv(folder_csv+'mot_jan22-hidalgo.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jean Lassalle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/8 n'ont pas de transcript. Peut-être les sous-titres furent-ils désactivés\n"
     ]
    }
   ],
   "source": [
    "indice_candidat = 6\n",
    "fichier_vidz = dossier_source + 'auto_candidat_' + str(indice_candidat) + '.csv'\n",
    "dossier_ts = transcripts_folder + 'JeanLassalle'\n",
    "df_candidat_2 = pd.read_csv(fichier_vidz)\n",
    "\n",
    "# Voir si les vidéos considérées ont bien des transcripts\n",
    "liste_videoid_sans_transcript, liste_videoid_avec_transcript = check_exhau_transcript(dtf_candidat=df_candidat_2, dossier_ts_associe = dossier_ts)\n",
    "# Croiser la liste des vidéos et les fichiers de transcripts pour créer la logorrhee (gros texte de tous les mots de tous les transcripts considérés)\n",
    "logoR_2 = creer_logorrhee_candidat(nom=liste_candidats[indice_candidat], dossierparent=transcripts_folder, liste_vidid_arecup = liste_videoid_avec_transcript)\n",
    "# Nettoyer la logorrhee\n",
    "cleanlog = NLTK_processing(logorrhee=logoR_2)\n",
    "\n",
    "# Nettoyer un peu les scories et faire un datafram avec [mot | nb occurences]\n",
    "data_dict = wordFrequency(cleanlog)\n",
    "data_items = data_dict. items()\n",
    "data_list = list(data_items)\n",
    "df = pd. DataFrame(data_list, columns=['mot', 'occurences']) \n",
    "\n",
    "df = df.sort_values(by = ['occurences'], ascending=False)\n",
    "df_to_export = df.head(20)\n",
    "\n",
    "# Dump des fichiers pour https://s01.810.fr\n",
    "dump_top_mots_candidat(dtf_synth_courant=df_to_export, nomfichier=folder_json+'mot_fev22-lassalle.json')\n",
    "df_to_export.to_csv(folder_csv+'mot_fev22-lassalle.csv')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2ed17ee1431ea646ff4cd0e36ebc6074f1354fab28092324062941991356fcdc"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
